{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from joblib import dump, load\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "import pickle as pkl\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "class append:\n",
    "    def __init__(self,df2):\n",
    "\n",
    "        self.df = df2 \n",
    "\n",
    "    def name(self, data):\n",
    "\n",
    "        t1 = ''\n",
    "        for i in range(0,8):\n",
    "            t1 += data[i]\n",
    "        parts2 = t1.str.split(':')\n",
    "        t2 = parts2.str[6]\n",
    "        parts3 = t2.str.split('\\'')\n",
    "        name = parts3.str[1]\n",
    "        return name\n",
    "    \n",
    "\n",
    "    def designation(self, data):\n",
    "\n",
    "        specific = 'name_limited'\n",
    "        contains_string = self.parts1.apply(lambda x: specific in x[4] if len(x) > 4 else False)\n",
    "\n",
    "        if contains_string.any():\n",
    "            name_limited = data[4]\n",
    "            designation = data[5]\n",
    "            nasa_jpl_url = data[6]\n",
    "            abs_magnitude_h = data[7]\n",
    "            diameter = ' '\n",
    "            for i in range(8,16):\n",
    "                diameter += data[i] + ','\n",
    "            hazard = data[16]\n",
    "        else:\n",
    "            name_limited = ' '\n",
    "            designation = data[4]\n",
    "            nasa_jpl_url = data[5]\n",
    "            abs_magnitude_h = data[6]\n",
    "            diameter = ' '\n",
    "            for i in range(7,15):\n",
    "                diameter += data[i] + ','\n",
    "            hazard = data[15]\n",
    "\n",
    "\n",
    "        return name_limited, designation, nasa_jpl_url, abs_magnitude_h, diameter, hazard\n",
    "    \n",
    "    def orbit(self, data):\n",
    "\n",
    "        orbit = ' '\n",
    "        orbit = data[1]\n",
    "        parts = orbit.str.split(',')\n",
    "        orbit_data1 = ' '\n",
    "        for i in range(1,26):\n",
    "            orbit_data1 += parts.str[i].astype(str) + ','\n",
    "        orbit_data1 = orbit_data1.str.rstrip(',')\n",
    "        parts2 = orbit_data1.str.split(':')\n",
    "        orbit_data = ' '\n",
    "        for i in range(1,30):\n",
    "            orbit_data += parts2.str[i].astype(str) + ':'\n",
    "        orbit_data = orbit_data.str.rstrip(':')\n",
    "        #orbit_data = orbit_data.apply(ast.literal_eval)\n",
    "\n",
    "        is_sentry_object1 = ' '\n",
    "        is_sentry_object1 = parts.str[26]\n",
    "        parts1 = is_sentry_object1.str.split(':')\n",
    "        is_sentry_object = parts1.str[1].astype(str)\n",
    "        is_sentry_object = is_sentry_object.str.rstrip('}')\n",
    "        is_sentry_object = is_sentry_object.str.lower() == 'true'\n",
    "        is_sentry_object = is_sentry_object.astype(bool)\n",
    "\n",
    "        return orbit_data, is_sentry_object\n",
    "\n",
    "    def separation(self):\n",
    "    \n",
    "\n",
    "        # Assuming df2 is already defined\n",
    "        df_result = pd.DataFrame()  # Initialize an empty DataFrame to store results\n",
    "\n",
    "        for i in range(len(self.df)):\n",
    "            df2t = self.df.iloc[[i]]  # Process each row\n",
    "            df2t = pd.DataFrame(df2t)\n",
    "            df2t['data'] = df2t['neo_data']\n",
    "            df2t = df2t.drop(columns=['neo_data'])\n",
    "            df2t = df2t.astype(str)\n",
    "\n",
    "            parts = df2t['data'].str.split(' ')\n",
    "            parts1 = df2t['data'].str.split(',')\n",
    "            partsorbit = df2t['data'].str.split(']')\n",
    "            self.parts1 = parts1\n",
    "            name_limited, designation, nasa_jpl_url, abs_magnitude_h, diameter, hazard = self.designation(parts1.str)\n",
    "            orbit_data, is_sentry_object = self.orbit(partsorbit.str)\n",
    "            \n",
    "            df2t['links'] = parts.str[1] + parts.str[2]\n",
    "            df2t['id'] = parts.str[4]\n",
    "            df2t['id'] = df2t['id'].str.replace('\\'', '')\n",
    "            df2t['id'] = df2t['id'].str.replace(',', '')\n",
    "            df2t['id'] = df2t['id'].astype(int)\n",
    "\n",
    "\n",
    "            df2t['neo_reference_id'] = parts.str[6]\n",
    "            df2t['neo_reference_id'] = df2t['neo_reference_id'].str.replace('\\'', '')\n",
    "            df2t['neo_reference_id'] = df2t['neo_reference_id'].str.replace(',', '')\n",
    "            df2t['neo_reference_id'] = df2t['neo_reference_id'].astype(int)\n",
    "\n",
    "\n",
    "            df2t['name'] = self.name(parts1.str)\n",
    "\n",
    "\n",
    "            df2t['name_limited'] = name_limited\n",
    "            parts = df2t['name_limited'].str.split(' ')\n",
    "            df2t['name_limited'] = parts.str[2]\n",
    "            df2t['name_limited'] = df2t['name_limited'].astype(str)\n",
    "            df2t['name_limited'] = df2t['name_limited'].str.replace('\\'', '')\n",
    "\n",
    "\n",
    "            df2t['designation'] = designation\n",
    "            parts = df2t['designation'].str.split(' ')\n",
    "            df2t['designation'] = parts.str[2]\n",
    "            df2t['designation'] = df2t['designation'].astype(str)\n",
    "            df2t['designation'] = df2t['designation'].str.replace('\\'', '')\n",
    "\n",
    "\n",
    "            df2t['nasa_jpl_url'] = nasa_jpl_url\n",
    "            parts = df2t['nasa_jpl_url'].str.split(' ')\n",
    "            df2t['nasa_jpl_url'] = parts.str[2]\n",
    "            df2t['nasa_jpl_url'] = df2t['nasa_jpl_url'].astype(str)\n",
    "            df2t['nasa_jpl_url'] = df2t['nasa_jpl_url'].str.replace('\\'', '')\n",
    "\n",
    "\n",
    "            df2t['absolute_magnitude_h'] = abs_magnitude_h\n",
    "            parts = df2t['absolute_magnitude_h'].str.split(' ')\n",
    "            df2t['absolute_magnitude_h'] = parts.str[2]\n",
    "            df2t['absolute_magnitude_h'] = df2t['absolute_magnitude_h'].astype(str)\n",
    "            df2t['absolute_magnitude_h'] = df2t['absolute_magnitude_h'].str.replace('\\'', '')\n",
    "            df2t['absolute_magnitude_h'] = df2t['absolute_magnitude_h'].astype(float)\n",
    "\n",
    "\n",
    "            df2t['estiated_diameter1'] = diameter\n",
    "            df2t['estiated_diameter1'] = df2t['estiated_diameter1'].str.rstrip(',')\n",
    "            parts = df2t['estiated_diameter1'].str.split(' ')\n",
    "\n",
    "            df2t['estimated_diameter'] = ' '\n",
    "            for i in range(3,23):\n",
    "                df2t['estimated_diameter'] += parts.str[i] + ' '\n",
    "            df2t.drop(columns=['estiated_diameter1'], inplace=True)\n",
    "\n",
    "\n",
    "            df2t['is_potentially_hazardous_asteroid'] = hazard\n",
    "            parts = df2t['is_potentially_hazardous_asteroid'].str.split(' ')\n",
    "            df2t['is_potentially_hazardous_asteroid'] = parts.str[2]\n",
    "            df2t['is_potentially_hazardous_asteroid'] = df2t['is_potentially_hazardous_asteroid'].astype(str)\n",
    "            df2t['is_potentially_hazardous_asteroid'] = df2t['is_potentially_hazardous_asteroid'].str.lower() == 'true'\n",
    "            df2t['is_potentially_hazardous_asteroid'] = df2t['is_potentially_hazardous_asteroid'].astype(bool)\n",
    "\n",
    "            parts1 = df2t['data'].str.split('[')\n",
    "            df2t['approachdata'] = ' '\n",
    "            df2t['approachdata'] = parts1.str[1]\n",
    "            parts2 = df2t['approachdata'].str.split(']')\n",
    "            df2t['close_approach_data'] = ' '\n",
    "            df2t['close_approach_data'] = parts2.str[0]\n",
    "            df2t['close_approach_data'] = df2t['close_approach_data'].apply(lambda x: f'[{x}]')\n",
    "            df2t['close_approach_data'] = df2t['close_approach_data'].apply(ast.literal_eval)\n",
    "            # df2t['close_approach_data'] = df2t['close_approach_data'].apply(lambda x: ' ' if x == [] else x)\n",
    "            df2t = df2t[df2t['close_approach_data'].apply(lambda x: x != [])]\n",
    "\n",
    "            df2t.drop(columns=['approachdata'], inplace=True)\n",
    "\n",
    "\n",
    "            # df2t['close_approach_data'] = df2t['close_approach_data'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "\n",
    "            df2t['orbital_data'] = orbit_data\n",
    "\n",
    "\n",
    "            df2t['is_sentry_object'] = is_sentry_object\n",
    "            \n",
    "                        \n",
    "\n",
    "\n",
    "            df_result = pd.concat([df_result, df2t], ignore_index=True)  # Append to the result DataFrame\n",
    "\n",
    "        df_result = df_result.drop(columns=['data'])\n",
    "        df_result.dropna(subset=['close_approach_data'], inplace=True)\n",
    "        self.df_result = df_result\n",
    "\n",
    "        return self.df_result\n",
    "    \n",
    "    def concat(self):\n",
    "\n",
    "        df_result = self.separation()\n",
    "\n",
    "        df = pd.read_csv('/Volumes/Maestria/GitHub/NEO/NEO/API_test/neo_data.csv')\n",
    "\n",
    "        df = pd.concat([df, df_result], ignore_index=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "class OverallProcessor:\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def explode(self):\n",
    "\n",
    "        def explode_approach(self):\n",
    "            self.df['close_approach_data'] = self.df['close_approach_data'].apply(ast.literal_eval)\n",
    "            self.df = self.df.explode(\"close_approach_data\").reset_index(drop=True)\n",
    "            normalized_close_approach_data = pd.json_normalize(self.df['close_approach_data'])\n",
    "            self.df = pd.concat([self.df.drop(columns=['close_approach_data']), normalized_close_approach_data], axis=1)\n",
    "            \n",
    "        \n",
    "        def clean_diameter(self):\n",
    "                self.df.drop(columns=['neo_reference_id', 'name_limited', 'links', 'nasa_jpl_url'], inplace=True)\n",
    "                self.df['estimated_diameter'] = self.df['estimated_diameter'].str.replace('\\'', '')\n",
    "                self.df['estimated_diameter'] = self.df['estimated_diameter'].str.replace('{', '')\n",
    "                self.df['estimated_diameter'] = self.df['estimated_diameter'].str.replace('}', '')\n",
    "                parts = self.df['estimated_diameter'].str.split(',')\n",
    "                self.df['estimated_diameter'] = parts.str[0] + parts.str[1]\n",
    "                parts = self.df['estimated_diameter'].str.split(':')\n",
    "                self.df['estimated_diameter'] = parts.str[1] + parts.str[2] + parts.str[3] \n",
    "                parts = self.df['estimated_diameter'].str.split(' ')\n",
    "                self.df['estimated_diameter_min'] = parts.str[2]\n",
    "                self.df['estimated_diameter_max'] = parts.str[4]\n",
    "                self.df.drop(columns=['estimated_diameter'], inplace=True)\n",
    "\n",
    "\n",
    "        def clean_orbits(self):\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace('\\'', '')\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace('{', '')\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace('}', '')\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace(']', '')\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace('[', '')\n",
    "            parts = self.df['orbital_data'].str.split(',')\n",
    "            self.df['extracted_orbital_data'] = parts.str[7]+parts.str[10]+parts.str[12]+parts.str[15]+parts.str[17]\n",
    "            parts = self.df['extracted_orbital_data'].str.split(' ')\n",
    "            self.df['minimum_orbit_intersection'] = parts.str[2]\n",
    "            self.df['eccentricity'] = parts.str[4]\n",
    "            self.df['inclination'] = parts.str[6]\n",
    "            self.df['perihilion_distance'] = parts.str[8]\n",
    "            self.df['aphelion_distance'] = parts.str[10]\n",
    "            self.df.drop(columns=['orbital_data', 'extracted_orbital_data'], inplace=True)    \n",
    "\n",
    "        explode_approach(self)\n",
    "        clean_diameter(self)\n",
    "        clean_orbits(self)\n",
    "        return self.df\n",
    "    \n",
    "    def clean(self):\n",
    "\n",
    "        def explode_approach(self):\n",
    "            self.df['close_approach_data'] = self.df['close_approach_data'].apply(ast.literal_eval)\n",
    "            self.df = self.df.explode(\"close_approach_data\").reset_index(drop=True)\n",
    "            normalized_close_approach_data = pd.json_normalize(self.df['close_approach_data'])\n",
    "            self.df = pd.concat([self.df.drop(columns=['close_approach_data']), normalized_close_approach_data], axis=1)\n",
    "            \n",
    "        \n",
    "        def clean_diameter(self):\n",
    "                self.df.drop(columns=['neo_reference_id', 'name_limited', 'links', 'nasa_jpl_url'], inplace=True)\n",
    "                self.df['estimated_diameter'] = self.df['estimated_diameter'].str.replace('\\'', '')\n",
    "                self.df['estimated_diameter'] = self.df['estimated_diameter'].str.replace('{', '')\n",
    "                self.df['estimated_diameter'] = self.df['estimated_diameter'].str.replace('}', '')\n",
    "                parts = self.df['estimated_diameter'].str.split(',')\n",
    "                self.df['estimated_diameter'] = parts.str[0] + parts.str[1]\n",
    "                parts = self.df['estimated_diameter'].str.split(':')\n",
    "                self.df['estimated_diameter'] = parts.str[1] + parts.str[2] + parts.str[3] \n",
    "                parts = self.df['estimated_diameter'].str.split(' ')\n",
    "                self.df['estimated_diameter_min'] = parts.str[2]\n",
    "                self.df['estimated_diameter_max'] = parts.str[4]\n",
    "                self.df.drop(columns=['estimated_diameter'], inplace=True)\n",
    "\n",
    "\n",
    "        def clean_orbits(self):\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace('\\'', '')\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace('{', '')\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace('}', '')\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace(']', '')\n",
    "            self.df['orbital_data'] = self.df['orbital_data'].str.replace('[', '')\n",
    "            parts = self.df['orbital_data'].str.split(',')\n",
    "            self.df['extracted_orbital_data'] = parts.str[7]+parts.str[10]+parts.str[12]+parts.str[15]+parts.str[17]\n",
    "            parts = self.df['extracted_orbital_data'].str.split(' ')\n",
    "            self.df['minimum_orbit_intersection'] = parts.str[2]\n",
    "            self.df['eccentricity'] = parts.str[4]\n",
    "            self.df['inclination'] = parts.str[6]\n",
    "            self.df['perihilion_distance'] = parts.str[8]\n",
    "            self.df['aphelion_distance'] = parts.str[10]\n",
    "            self.df.drop(columns=['orbital_data', 'extracted_orbital_data'], inplace=True)   \n",
    "\n",
    "\n",
    "        def clean_df(self):\n",
    "            self.df.drop(columns=['id', 'name', 'designation', 'is_sentry_object', 'close_approach_date', 'close_approach_date_full', 'epoch_date_close_approach', 'orbiting_body', 'relative_velocity.kilometers_per_second', 'relative_velocity.miles_per_hour', 'miss_distance.astronomical', 'miss_distance.lunar', 'miss_distance.miles' ], inplace=True)\n",
    "            self.df = self.df.rename(columns={'is_potentially_hazardous_asteroid': 'is_hazardous'}) \n",
    "            estimated_diameter_average = (self.df['estimated_diameter_min'].astype(float) + self.df['estimated_diameter_max'].astype(float)) / 2\n",
    "            self.df['estimated_diameter_average'] = estimated_diameter_average\n",
    "\n",
    "        def encoder(self):\n",
    "            le = LabelEncoder()\n",
    "            self.df['is_hazardous'] = le.fit_transform(self.df['is_hazardous'])\n",
    "\n",
    "        explode_approach(self)\n",
    "        clean_diameter(self)\n",
    "        clean_orbits(self)\n",
    "        clean_df(self)\n",
    "        encoder(self)\n",
    "        return self.df  \n",
    "    \n",
    "    def smote(self):\n",
    "             \n",
    "        self.df_test = self.df.copy()\n",
    "\n",
    "        sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "        oversampled_X, oversampled_Y = sm.fit_resample(self.df_test.drop('is_hazardous', axis=1), self.df_test['is_hazardous'])\n",
    "        self.df = pd.concat([pd.DataFrame(oversampled_Y), pd.DataFrame(oversampled_X)], axis=1)\n",
    "        return self.df\n",
    "    \n",
    "\n",
    "class scalesplit:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def ssplit(self):\n",
    "\n",
    "        def split(self):\n",
    "\n",
    "         X = self.df.drop(columns=['is_hazardous'])\n",
    "         y = self.df['is_hazardous']\n",
    "\n",
    "\n",
    "         split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "         for train_index, test_index in split.split(X, y):\n",
    "          self.X_train, self.X_test = X.loc[train_index], X.loc[test_index]\n",
    "          self.y_train, self.y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "         \n",
    "\n",
    "        def scale(self):\n",
    "         \n",
    "            self.X_train = StandardScaler().fit_transform(self.X_train)\n",
    "            self.X_test = StandardScaler().fit_transform(self.X_test)\n",
    "\n",
    "    \n",
    "        split(self)\n",
    "        scale(self)\n",
    "\n",
    "        X_train = self.X_train\n",
    "        X_test = self.X_test\n",
    "        y_train = self.y_train\n",
    "        y_test = self.y_test\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "class LogRegression:\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def Regression(self):\n",
    "\n",
    "            def gridsearch():\n",
    "                logReg = LogisticRegression()\n",
    "\n",
    "                param_grid = {'solver': ['liblinear', 'newton-cholesky'],\n",
    "              'penalty':['none', 'l2'],\n",
    "              'C':[0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'n_jobs': [8],\n",
    "              'random_state': [0, 42, 32],\n",
    "              'fit_intercept': [True, False],\n",
    "              'warm_start': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "                grid_search = GridSearchCV(logReg, param_grid, cv=5, verbose=0, n_jobs=-1)\n",
    "                grid_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "\n",
    "                self.best_estimator_ = LogisticRegression(C=0.001, fit_intercept=False, n_jobs=8, random_state=0,solver='liblinear', warm_start=True) \n",
    "\n",
    "\n",
    "                print(f'Best parameters: {grid_search.best_params_}')\n",
    "                print(f'Best Score: {grid_search.best_score_}')\n",
    "                print(f'Best Estimator: {grid_search.best_estimator_} ')\n",
    "\n",
    "            def fit(self):\n",
    "\n",
    "                best_model = self.best_estimator_\n",
    "                best_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "                prediction = best_model.predict(self.X_test)\n",
    "\n",
    "                accuracy = accuracy_score(self.y_test, prediction)\n",
    "                recall = recall_score(prediction, self.y_test)\n",
    "                f1 = f1_score(prediction, self.y_test)\n",
    "\n",
    "                print (f'The accuracy score is {accuracy}, The recall score is {recall}, The f1 score is {f1}')\n",
    "\n",
    "                print(f'Classification Report: \\n {classification_report(self.y_test, prediction)}')\n",
    "\n",
    "                cm = confusion_matrix(self.y_test, prediction)\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "                disp.plot(cmap='Blues')\n",
    "                plt.show()\n",
    "\n",
    "                cm2 = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "                sns.heatmap(cm2, annot=True, cmap='Blues')\n",
    "                \n",
    "            gridsearch()\n",
    "            fit(self)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
